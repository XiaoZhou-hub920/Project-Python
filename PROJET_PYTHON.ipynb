{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 1 (TD 3 à 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longueur du corpus : 200\n",
      "Moyenne du nombre de phrases : 6.17\n",
      "Moyenne du nombre de mots : 102.92\n",
      "Nombre total de mots dans le corpus : 20583\n",
      "Date actuelle : 2025-01-03 21:58:23.026921\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import urllib.request\n",
    "import xmltodict\n",
    "import numpy as np\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, titre=\"\", auteur=\"\", date=\"\", url=\"\", texte=\"\"):\n",
    "        self.titre = titre\n",
    "        self.auteur = auteur\n",
    "        self.date = date\n",
    "        self.url = url\n",
    "        self.texte = texte\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Titre : {self.titre}\\tAuteur : {self.auteur}\\tDate : {self.date}\\tURL : {self.url}\\tTexte : {self.texte}\\t\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.titre}, par {self.auteur}\"\n",
    "\n",
    "class Author:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.ndoc = 0\n",
    "        self.production = []\n",
    "\n",
    "    def add(self, production):\n",
    "        self.ndoc += 1\n",
    "        self.production.append(production)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Auteur : {self.name}\\t# productions : {self.ndoc}\"\n",
    "\n",
    "class CorpusBuilder:\n",
    "    def __init__(self):\n",
    "        self.textes_reddit = []\n",
    "        self.textes_arxiv = []\n",
    "        self.corpus = []\n",
    "\n",
    "    def fetch_reddit_data(self, client_id, client_secret, user_agent, subreddit=\"Coronavirus\", limit=100):\n",
    "        \"\"\"\n",
    "        Collecte les données du subreddit spécifié en utilisant l'API Reddit.\n",
    "        \"\"\"\n",
    "        reddit = praw.Reddit(client_id='wiQgc72eQSxNz0bB2Km-xw', client_secret='bzlnKqXlJltn2zixvCXxPc0_f3aHTg', user_agent='BEYE')\n",
    "        subr = reddit.subreddit(subreddit)\n",
    "\n",
    "        for post in subr.hot(limit=limit):\n",
    "            texte = post.title.replace(\"\\n\", \" \")\n",
    "            self.textes_reddit.append(Document(titre=post.title, texte=texte, auteur=str(post.author), date=datetime.datetime.fromtimestamp(post.created_utc).strftime(\"%Y-%m-%d\")))\n",
    "\n",
    "    def fetch_arxiv_data(self, query=\"covid\", max_results=100):\n",
    "        \"\"\"\n",
    "        Collecte les données d'Arxiv en fonction de la requête spécifiée.\n",
    "        \"\"\"\n",
    "        url = f'http://export.arxiv.org/api/query?search_query=all:{query}&start=0&max_results={max_results}'\n",
    "        url_read = urllib.request.urlopen(url).read()\n",
    "        data = url_read.decode()\n",
    "        dico = xmltodict.parse(data)\n",
    "\n",
    "        docs = dico['feed']['entry'] if isinstance(dico['feed']['entry'], list) else [dico['feed']['entry']]\n",
    "        for d in docs:\n",
    "            texte = f\"{d['title']}. {d['summary']}\".replace(\"\\n\", \" \")\n",
    "            auteur = d.get('author', {}).get('name', 'Unknown') if isinstance(d.get('author'), dict) else ', '.join(a['name'] for a in d['author'])\n",
    "            self.textes_arxiv.append(Document(titre=d['title'], texte=texte, auteur=auteur, date=d.get('published', 'Unknown')))\n",
    "\n",
    "    def build_corpus(self):\n",
    "        \"\"\"\n",
    "        Combine les données collectées de Reddit et d'Arxiv en un corpus unique.\n",
    "        \"\"\"\n",
    "        self.corpus = self.textes_reddit + self.textes_arxiv\n",
    "        return self.corpus\n",
    "\n",
    "    def compute_statistics(self):\n",
    "        \"\"\"\n",
    "        Calcule des statistiques sur le corpus, telles que la moyenne et le total de mots et phrases.\n",
    "        \"\"\"\n",
    "        nb_phrases = [len(doc.texte.split(\".\")) for doc in self.corpus]\n",
    "        nb_mots = [len(doc.texte.split(\" \")) for doc in self.corpus]\n",
    "\n",
    "        stats = {\n",
    "            \"longueur_corpus\": len(self.corpus),\n",
    "            \"moyenne_phrases\": np.mean(nb_phrases) if nb_phrases else 0,\n",
    "            \"moyenne_mots\": np.mean(nb_mots) if nb_mots else 0,\n",
    "            \"total_mots\": np.sum(nb_mots),\n",
    "        }\n",
    "        return stats\n",
    "\n",
    "    def filter_long_texts(self, min_length=100):\n",
    "        \"\"\"\n",
    "        Filtre les documents ayant un texte supérieur à une longueur minimale spécifiée.\n",
    "        \"\"\"\n",
    "        return [doc for doc in self.corpus if len(doc.texte) > min_length]\n",
    "\n",
    "    def save_corpus(self, corpus, filename=\"out.pkl\"):\n",
    "        \"\"\"\n",
    "        Sauvegarde le corpus filtré dans un fichier pickle.\n",
    "        \"\"\"\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(corpus, f)\n",
    "\n",
    "    def load_corpus(self, filename=\"out.pkl\"):\n",
    "        \"\"\"\n",
    "        Charge un corpus à partir d'un fichier pickle.\n",
    "        \"\"\"\n",
    "        with open(filename, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    builder = CorpusBuilder()\n",
    "\n",
    "    # Fetch data (replace placeholders with actual Reddit credentials)\n",
    "    builder.fetch_reddit_data(client_id=\"***identifiant***\", client_secret=\"***motdepasse***\", user_agent=\"***nom***\")\n",
    "    builder.fetch_arxiv_data()\n",
    "\n",
    "    # Build and analyze corpus\n",
    "    corpus = builder.build_corpus()\n",
    "    stats = builder.compute_statistics()\n",
    "\n",
    "    print(f\"Longueur du corpus : {stats['longueur_corpus']}\")\n",
    "    print(f\"Moyenne du nombre de phrases : {stats['moyenne_phrases']:.2f}\")\n",
    "    print(f\"Moyenne du nombre de mots : {stats['moyenne_mots']:.2f}\")\n",
    "    print(f\"Nombre total de mots dans le corpus : {stats['total_mots']}\")\n",
    "\n",
    "    # Filter and save corpus\n",
    "    filtered_corpus = builder.filter_long_texts()\n",
    "    builder.save_corpus(filtered_corpus)\n",
    "\n",
    "    # Display current date\n",
    "    print(f\"Date actuelle : {datetime.datetime.now()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERSION 2 (TD 3 à 7 : avec le moteur de recherche)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longueur du corpus : 200\n",
      "Moyenne du nombre de phrases : 6.17\n",
      "Moyenne du nombre de mots : 102.92\n",
      "Nombre total de mots dans le corpus : 20583\n",
      "\n",
      "Top résultats pour votre requête :\n",
      "\n",
      "1. Houston researchers make nasal vaccine that prevents COVID from spreading (Score: 0.2773)\n",
      "   Auteur: rednoise\n",
      "   Date: 2024-08-11\n",
      "   Extrait: Houston researchers make nasal vaccine that prevents COVID from spreading\n",
      "\n",
      "2. NIH-sponsored trial of nasal COVID-19 vaccine opens (Score: 0.2731)\n",
      "   Auteur: BothZookeepergame612\n",
      "   Date: 2024-12-26\n",
      "   Extrait: NIH-sponsored trial of nasal COVID-19 vaccine opens\n",
      "\n",
      "3. Does the updated COVID-19 vaccine protect against the XEC variant? (Score: 0.2726)\n",
      "   Auteur: CTVNEWS\n",
      "   Date: 2024-10-23\n",
      "   Extrait: Does the updated COVID-19 vaccine protect against the XEC variant?\n",
      "\n",
      "4. You're More Likely to Get Heart Issues from COVID-19 Than the Vaccine (Score: 0.2616)\n",
      "   Auteur: burtzev\n",
      "   Date: 2024-08-31\n",
      "   Extrait: You're More Likely to Get Heart Issues from COVID-19 Than the Vaccine\n",
      "\n",
      "5. What the end of the CDC's COVID vaccine access program means for uninsured Americans (Score: 0.2362)\n",
      "   Auteur: I_who_have_no_need\n",
      "   Date: 2024-08-29\n",
      "   Extrait: What the end of the CDC's COVID vaccine access program means for uninsured Americans\n",
      "\n",
      "Date actuelle : 2025-01-03 21:58:25.479037\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import urllib.request\n",
    "import xmltodict\n",
    "import numpy as np\n",
    "import pickle\n",
    "import datetime\n",
    "import re\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, titre=\"\", auteur=\"\", date=\"\", url=\"\", texte=\"\"):\n",
    "        self.titre = titre\n",
    "        self.auteur = auteur\n",
    "        self.date = date\n",
    "        self.url = url\n",
    "        self.texte = texte\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Titre : {self.titre}\\tAuteur : {self.auteur}\\tDate : {self.date}\\tURL : {self.url}\\tTexte : {self.texte}\\t\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.titre}, par {self.auteur}\"\n",
    "\n",
    "class CorpusBuilder:\n",
    "    def __init__(self):\n",
    "        self.textes_reddit = []\n",
    "        self.textes_arxiv = []\n",
    "        self.corpus = []\n",
    "        self.vocabulaire = []\n",
    "        self.tf_matrix = None\n",
    "        self.tf_idf_matrix = None\n",
    "\n",
    "    def fetch_reddit_data(self, client_id, client_secret, user_agent, subreddit=\"Coronavirus\", limit=100):\n",
    "        \"\"\"\n",
    "        Collecte les données du subreddit spécifié en utilisant l'API Reddit.\n",
    "        \"\"\"\n",
    "        reddit = praw.Reddit(client_id='wiQgc72eQSxNz0bB2Km-xw', client_secret='bzlnKqXlJltn2zixvCXxPc0_f3aHTg', user_agent='BEYE')\n",
    "        subr = reddit.subreddit(subreddit)\n",
    "\n",
    "        for post in subr.hot(limit=limit):\n",
    "            texte = post.title.replace(\"\\n\", \" \")\n",
    "            self.textes_reddit.append(Document(titre=post.title, texte=texte, auteur=str(post.author), date=datetime.datetime.fromtimestamp(post.created_utc).strftime(\"%Y-%m-%d\")))\n",
    "\n",
    "    def fetch_arxiv_data(self, query=\"covid\", max_results=100):\n",
    "        \"\"\"\n",
    "        Collecte les données d'Arxiv en fonction de la requête spécifiée.\n",
    "        \"\"\"\n",
    "        url = f'http://export.arxiv.org/api/query?search_query=all:{query}&start=0&max_results={max_results}'\n",
    "        url_read = urllib.request.urlopen(url).read()\n",
    "        data = url_read.decode()\n",
    "        dico = xmltodict.parse(data)\n",
    "\n",
    "        docs = dico['feed']['entry'] if isinstance(dico['feed']['entry'], list) else [dico['feed']['entry']]\n",
    "        for d in docs:\n",
    "            texte = f\"{d['title']}. {d['summary']}\".replace(\"\\n\", \" \")\n",
    "            auteur = d.get('author', {}).get('name', 'Unknown') if isinstance(d.get('author'), dict) else ', '.join(a['name'] for a in d['author'])\n",
    "            self.textes_arxiv.append(Document(titre=d['title'], texte=texte, auteur=auteur, date=d.get('published', 'Unknown')))\n",
    "\n",
    "    def build_corpus(self):\n",
    "        \"\"\"\n",
    "        Combine les données collectées de Reddit et d'Arxiv en un corpus unique.\n",
    "        \"\"\"\n",
    "        self.corpus = self.textes_reddit + self.textes_arxiv\n",
    "        return self.corpus\n",
    "\n",
    "    def compute_statistics(self):\n",
    "        \"\"\"\n",
    "        Calcule des statistiques sur le corpus, telles que la moyenne et le total de mots et phrases.\n",
    "        \"\"\"\n",
    "        nb_phrases = [len(doc.texte.split(\".\")) for doc in self.corpus]\n",
    "        nb_mots = [len(doc.texte.split(\" \")) for doc in self.corpus]\n",
    "\n",
    "        stats = {\n",
    "            \"longueur_corpus\": len(self.corpus),\n",
    "            \"moyenne_phrases\": np.mean(nb_phrases) if nb_phrases else 0,\n",
    "            \"moyenne_mots\": np.mean(nb_mots) if nb_mots else 0,\n",
    "            \"total_mots\": np.sum(nb_mots),\n",
    "        }\n",
    "        return stats\n",
    "\n",
    "    def construire_vocabulaire_et_tf(self):\n",
    "        \"\"\"\n",
    "        Construit le vocabulaire et la matrice TF (Term Frequency) pour le corpus.\n",
    "        \"\"\"\n",
    "        vocabulaire = {}\n",
    "        rows, cols, data = [], [], []\n",
    "\n",
    "        for doc_id, doc in enumerate(self.corpus):\n",
    "            mots = re.findall(r'\\b\\w+\\b', doc.texte.lower())\n",
    "            compte_mots = Counter(mots)\n",
    "\n",
    "            for mot, freq in compte_mots.items():\n",
    "                if mot not in vocabulaire:\n",
    "                    vocabulaire[mot] = len(vocabulaire)\n",
    "\n",
    "                rows.append(doc_id)\n",
    "                cols.append(vocabulaire[mot])\n",
    "                data.append(freq)\n",
    "\n",
    "        self.vocabulaire = vocabulaire\n",
    "        self.tf_matrix = csr_matrix((data, (rows, cols)), shape=(len(self.corpus), len(vocabulaire)))\n",
    "        return self.tf_matrix\n",
    "\n",
    "    def construire_tf_idf(self):\n",
    "        \"\"\"\n",
    "        Construit la matrice TF-IDF pour le corpus.\n",
    "        \"\"\"\n",
    "        n_docs = self.tf_matrix.shape[0]\n",
    "        idf = np.log((1 + n_docs) / (1 + (self.tf_matrix > 0).sum(axis=0))) + 1\n",
    "        self.tf_idf_matrix = self.tf_matrix.multiply(idf.A1)\n",
    "        return self.tf_idf_matrix\n",
    "\n",
    "    def recherche(self, requete, top_n=5):\n",
    "        \"\"\"\n",
    "        Recherche les documents correspondant aux mots-clés de la requête en utilisant la similarité cosinus.\n",
    "        Retourne les `top_n` résultats avec une mise en forme améliorée.\n",
    "        \"\"\"\n",
    "        mots_recherche = re.findall(r'\\b\\w+\\b', requete.lower())\n",
    "        vecteur_requete = np.zeros(len(self.vocabulaire))\n",
    "\n",
    "        for mot in mots_recherche:\n",
    "            if mot in self.vocabulaire:\n",
    "                vecteur_requete[self.vocabulaire[mot]] += 1\n",
    "\n",
    "        scores = []\n",
    "        for doc_id in range(self.tf_idf_matrix.shape[0]):\n",
    "            doc_vecteur = self.tf_idf_matrix.getrow(doc_id).toarray().flatten()\n",
    "            score = 1 - cosine(vecteur_requete, doc_vecteur) if np.any(doc_vecteur) else 0\n",
    "            scores.append((doc_id, score))\n",
    "\n",
    "        scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "        resultats = [(self.corpus[doc_id], score) for doc_id, score in scores if score > 0]\n",
    "\n",
    "        if not resultats:\n",
    "            return \"Aucun résultat trouvé pour votre requête.\"\n",
    "\n",
    "        # Mise en forme des résultats\n",
    "        output = \"\\nTop résultats pour votre requête :\\n\"\n",
    "        for i, (doc, score) in enumerate(resultats[:top_n]):\n",
    "            extrait = doc.texte[:200] + \"...\" if len(doc.texte) > 200 else doc.texte\n",
    "            output += f\"\\n{i+1}. {doc.titre} (Score: {score:.4f})\\n   Auteur: {doc.auteur}\\n   Date: {doc.date}\\n   Extrait: {extrait}\\n\"\n",
    "        return output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    builder = CorpusBuilder()\n",
    "\n",
    "    # Fetch data (replace placeholders with actual Reddit credentials)\n",
    "    builder.fetch_reddit_data(client_id=\"***identifiant***\", client_secret=\"***motdepasse***\", user_agent=\"***nom***\")\n",
    "    builder.fetch_arxiv_data()\n",
    "\n",
    "    # Build and analyze corpus\n",
    "    corpus = builder.build_corpus()\n",
    "    stats = builder.compute_statistics()\n",
    "\n",
    "    print(f\"Longueur du corpus : {stats['longueur_corpus']}\")\n",
    "    print(f\"Moyenne du nombre de phrases : {stats['moyenne_phrases']:.2f}\")\n",
    "    print(f\"Moyenne du nombre de mots : {stats['moyenne_mots']:.2f}\")\n",
    "    print(f\"Nombre total de mots dans le corpus : {stats['total_mots']}\")\n",
    "\n",
    "    # Construire vocabulaire, matrice TF, et matrice TF-IDF\n",
    "    builder.construire_vocabulaire_et_tf()\n",
    "    builder.construire_tf_idf()\n",
    "\n",
    "    # Recherche dans le corpus\n",
    "    requete = \"covid vaccine\"\n",
    "    resultats = builder.recherche(requete)\n",
    "    print(resultats)\n",
    "\n",
    "    # Display current date\n",
    "    print(f\"Date actuelle : {datetime.datetime.now()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERSION 3 (TD 3 à 7 + TD 8 à 10 : avec l'interface et l'extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import urllib.request\n",
    "import xmltodict\n",
    "import numpy as np\n",
    "import pickle\n",
    "import datetime\n",
    "import re\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial.distance import cosine\n",
    "import streamlit as st\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, titre=\"\", auteur=\"\", date=\"\", url=\"\", texte=\"\"):\n",
    "        self.titre = titre\n",
    "        self.auteur = auteur\n",
    "        self.date = date\n",
    "        self.url = url\n",
    "        self.texte = texte\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Titre : {self.titre}\\tAuteur : {self.auteur}\\tDate : {self.date}\\tURL : {self.url}\\tTexte : {self.texte}\\t\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.titre}, par {self.auteur}\"\n",
    "\n",
    "class CorpusBuilder:\n",
    "    def __init__(self):\n",
    "        self.textes_reddit = []\n",
    "        self.textes_arxiv = []\n",
    "        self.corpus = []\n",
    "        self.vocabulaire = []\n",
    "        self.tf_matrix = None\n",
    "        self.tf_idf_matrix = None\n",
    "\n",
    "    def fetch_reddit_data(self, client_id, client_secret, user_agent, subreddit=\"Coronavirus\", limit=100):\n",
    "        \"\"\"\n",
    "        Collecte les données du subreddit spécifié en utilisant l'API Reddit.\n",
    "        \"\"\"\n",
    "        reddit = praw.Reddit(client_id='wiQgc72eQSxNz0bB2Km-xw', client_secret='bzlnKqXlJltn2zixvCXxPc0_f3aHTg', user_agent='BEYE')\n",
    "        subr = reddit.subreddit(subreddit)\n",
    "\n",
    "        for post in subr.hot(limit=limit):\n",
    "            texte = post.title.replace(\"\\n\", \" \")\n",
    "            self.textes_reddit.append(Document(titre=post.title, texte=texte, auteur=str(post.author), date=datetime.datetime.fromtimestamp(post.created_utc).strftime(\"%Y-%m-%d\")))\n",
    "\n",
    "    def fetch_arxiv_data(self, query=\"covid\", max_results=100):\n",
    "        \"\"\"\n",
    "        Collecte les données d'Arxiv en fonction de la requête spécifiée.\n",
    "        \"\"\"\n",
    "        url = f'http://export.arxiv.org/api/query?search_query=all:{query}&start=0&max_results={max_results}'\n",
    "        url_read = urllib.request.urlopen(url).read()\n",
    "        data = url_read.decode()\n",
    "        dico = xmltodict.parse(data)\n",
    "\n",
    "        docs = dico['feed']['entry'] if isinstance(dico['feed']['entry'], list) else [dico['feed']['entry']]\n",
    "        for d in docs:\n",
    "            texte = f\"{d['title']}. {d['summary']}\".replace(\"\\n\", \" \")\n",
    "            auteur = d.get('author', {}).get('name', 'Unknown') if isinstance(d.get('author'), dict) else ', '.join(a['name'] for a in d['author'])\n",
    "            self.textes_arxiv.append(Document(titre=d['title'], texte=texte, auteur=auteur, date=d.get('published', 'Unknown')))\n",
    "\n",
    "    def build_corpus(self):\n",
    "        \"\"\"\n",
    "        Combine les données collectées de Reddit et d'Arxiv en un corpus unique.\n",
    "        \"\"\"\n",
    "        self.corpus = self.textes_reddit + self.textes_arxiv\n",
    "        return self.corpus\n",
    "\n",
    "    def compute_statistics(self):\n",
    "        \"\"\"\n",
    "        Calcule des statistiques sur le corpus, telles que la moyenne et le total de mots et phrases.\n",
    "        \"\"\"\n",
    "        nb_phrases = [len(doc.texte.split(\".\")) for doc in self.corpus]\n",
    "        nb_mots = [len(doc.texte.split(\" \")) for doc in self.corpus]\n",
    "\n",
    "        stats = {\n",
    "            \"longueur_corpus\": len(self.corpus),\n",
    "            \"moyenne_phrases\": np.mean(nb_phrases) if nb_phrases else 0,\n",
    "            \"moyenne_mots\": np.mean(nb_mots) if nb_mots else 0,\n",
    "            \"total_mots\": np.sum(nb_mots),\n",
    "        }\n",
    "        return stats\n",
    "\n",
    "    def construire_vocabulaire_et_tf(self):\n",
    "        \"\"\"\n",
    "        Construit le vocabulaire et la matrice TF (Term Frequency) pour le corpus.\n",
    "        \"\"\"\n",
    "        vocabulaire = {}\n",
    "        rows, cols, data = [], [], []\n",
    "\n",
    "        for doc_id, doc in enumerate(self.corpus):\n",
    "            mots = re.findall(r'\\b\\w+\\b', doc.texte.lower())\n",
    "            compte_mots = Counter(mots)\n",
    "\n",
    "            for mot, freq in compte_mots.items():\n",
    "                if mot not in vocabulaire:\n",
    "                    vocabulaire[mot] = len(vocabulaire)\n",
    "\n",
    "                rows.append(doc_id)\n",
    "                cols.append(vocabulaire[mot])\n",
    "                data.append(freq)\n",
    "\n",
    "        self.vocabulaire = vocabulaire\n",
    "        self.tf_matrix = csr_matrix((data, (rows, cols)), shape=(len(self.corpus), len(vocabulaire)))\n",
    "        return self.tf_matrix\n",
    "\n",
    "    def construire_tf_idf(self):\n",
    "        \"\"\"\n",
    "        Construit la matrice TF-IDF pour le corpus.\n",
    "        \"\"\"\n",
    "        n_docs = self.tf_matrix.shape[0]\n",
    "        idf = np.log((1 + n_docs) / (1 + (self.tf_matrix > 0).sum(axis=0))) + 1\n",
    "        self.tf_idf_matrix = self.tf_matrix.multiply(idf.A1)\n",
    "        return self.tf_idf_matrix\n",
    "\n",
    "    def recherche(self, requete, top_n=5):\n",
    "        \"\"\"\n",
    "        Recherche les documents correspondant aux mots-clés de la requête en utilisant la similarité cosinus.\n",
    "        Retourne les `top_n` résultats avec une mise en forme améliorée.\n",
    "        \"\"\"\n",
    "        mots_recherche = re.findall(r'\\b\\w+\\b', requete.lower())\n",
    "        vecteur_requete = np.zeros(len(self.vocabulaire))\n",
    "\n",
    "        for mot in mots_recherche:\n",
    "            if mot in self.vocabulaire:\n",
    "                vecteur_requete[self.vocabulaire[mot]] += 1\n",
    "\n",
    "        scores = []\n",
    "        for doc_id in range(self.tf_idf_matrix.shape[0]):\n",
    "            doc_vecteur = self.tf_idf_matrix.getrow(doc_id).toarray().flatten()\n",
    "            score = 1 - cosine(vecteur_requete, doc_vecteur) if np.any(doc_vecteur) else 0\n",
    "            scores.append((doc_id, score))\n",
    "\n",
    "        scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "        resultats = [(self.corpus[doc_id], score) for doc_id, score in scores if score > 0]\n",
    "\n",
    "        return resultats[:top_n]\n",
    "\n",
    "    def export_results(self, results):\n",
    "        \"\"\"\n",
    "        Exporte les résultats de la recherche en CSV.\n",
    "        \"\"\"\n",
    "        data = [{\n",
    "            \"Titre\": doc.titre,\n",
    "            \"Auteur\": doc.auteur,\n",
    "            \"Date\": doc.date,\n",
    "            \"Score\": score,\n",
    "            \"Extrait\": doc.texte[:200]\n",
    "        } for doc, score in results]\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "# Application Streamlit\n",
    "st.title(\"Moteur de Recherche - Corpus COVID\")\n",
    "\n",
    "st.sidebar.header(\"Paramètres\")\n",
    "subreddit = st.sidebar.text_input(\"Subreddit\", \"Coronavirus\")\n",
    "requete = st.text_input(\"Entrez votre requête\", \"\")\n",
    "\n",
    "tri_option = st.sidebar.selectbox(\"Trier les résultats par\", [\"Pertinence (Score)\", \"Date\", \"Auteur\"])\n",
    "\n",
    "if st.sidebar.button(\"Collecter les données\"):\n",
    "    builder = CorpusBuilder()\n",
    "    builder.fetch_reddit_data(client_id=\"***\", client_secret=\"***\", user_agent=\"***\", subreddit=subreddit)\n",
    "    builder.fetch_arxiv_data()\n",
    "    builder.build_corpus()\n",
    "    builder.construire_vocabulaire_et_tf()\n",
    "    builder.construire_tf_idf()\n",
    "\n",
    "    st.session_state[\"builder\"] = builder\n",
    "    st.success(\"Données collectées avec succès !\")\n",
    "\n",
    "if \"builder\" in st.session_state:\n",
    "    builder = st.session_state[\"builder\"]\n",
    "\n",
    "    # Affichage des statistiques\n",
    "    stats = builder.compute_statistics()\n",
    "    st.sidebar.subheader(\"Statistiques du Corpus\")\n",
    "    st.sidebar.write(f\"**Nombre de documents**: {stats['longueur_corpus']}\")\n",
    "    st.sidebar.write(f\"**Total de mots**: {stats['total_mots']}\")\n",
    "    st.sidebar.write(f\"**Moyenne de phrases**: {stats['moyenne_phrases']:.2f}\")\n",
    "\n",
    "    if requete:\n",
    "        st.header(f\"Résultats pour : {requete}\")\n",
    "        resultats = builder.recherche(requete)\n",
    "\n",
    "        if resultats:\n",
    "            # Tri des résultats\n",
    "            if tri_option == \"Date\":\n",
    "                resultats.sort(key=lambda x: x[0].date, reverse=True)\n",
    "            elif tri_option == \"Auteur\":\n",
    "                resultats.sort(key=lambda x: x[0].auteur)\n",
    "\n",
    "            # Graphique de scores\n",
    "            scores = [score for _, score in resultats]\n",
    "            titres = [doc.titre[:30] for doc, _ in resultats]\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.barh(titres[::-1], scores[::-1])\n",
    "            ax.set_xlabel(\"Scores de Similarité\")\n",
    "            ax.set_title(\"Scores des Résultats\")\n",
    "            st.pyplot(fig)\n",
    "\n",
    "            # Affichage des résultats\n",
    "            for i, (doc, score) in enumerate(resultats):\n",
    "                st.subheader(f\"{i+1}. {doc.titre}\")\n",
    "                st.write(f\"**Auteur** : {doc.auteur}\")\n",
    "                st.write(f\"**Date** : {doc.date}\")\n",
    "                st.write(f\"**Score** : {score:.4f}\")\n",
    "                st.write(f\"**Extrait** : {doc.texte[:200]}...\")\n",
    "\n",
    "            # Export des résultats\n",
    "            df_results = builder.export_results(resultats)\n",
    "            csv = df_results.to_csv(index=False).encode('utf-8')\n",
    "            st.download_button(\n",
    "                label=\"Télécharger les résultats au format CSV\",\n",
    "                data=csv,\n",
    "                file_name=\"resultats_recherche.csv\",\n",
    "                mime=\"text/csv\"\n",
    "            )\n",
    "        else:\n",
    "            st.warning(\"Aucun résultat trouvé.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
